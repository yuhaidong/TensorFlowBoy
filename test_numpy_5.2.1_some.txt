1 损失函数：
	损失函数用来刻画预测值与真实值之间的差异，使用上有3种情况：
		1 分类问题：一般采用“交叉熵+softmax回归”，见P75；
		2 回归问题：一般采用“均方误差”，见P77；
		3 自定义损失函数：还可以自定义损失函数，见P78

2 反向传播算法
	反向传播算法要用到损失函数，反向传播算法就是通过损失函数刻画出的测试值与真实值之间的差异，来对参数进行优化的。
	梯度下降算法主要用于优化单个参数的取值，而反向传播算法给出了一个高效的方式在所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小。
	反向传播算法会计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数，反向传播算法可以说是梯度下降在链式法则中的应用。

3 梯度下降算法
	梯度其实就是损失函数当前所在的位置，梯度下降算法，对参数的梯度通过求偏导的方式计算出来，再根据学习率来更新参数。
	这里面有两个概念：梯度，学习率。
	1 梯度：可以将梯度理解为斜率，通过求偏导得到。
	2 学习率：每次参数更新的幅度

4 随机梯度下降
	梯度下降算法，有两个缺点：1 不一定能够达到全局最优，2 计算时间太长。
	梯度下降算法，每轮训练会输入n次参数进行训练。如果每一轮训练都需要计算在全部训练数据上的损失函数，这是非常耗时的。
	随机梯度下降算法，优化的不是在全部训练数据上的损失函数，而是在每一轮迭代中，随机优化某一条训练数据上的损失函数。这样每一轮参数更新的速度大大加快。

5 batch
	随机梯度下降算法，每次优化的只是某一条数据上的损失函数，所以缺点也比较明显：
	在某一条数据上损失函数更新并不代表在全部数据上损失函数更小，于是使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优。

	于是为了综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中一般采用这两个算法的折中：
	每次计算一小部分训练数据的损失函数，这一小部分数据被称之为一个batch。

	通过矩阵运算，每次在一个batch上优化神经网络的参数并不会比单个数据慢太多。
	另一方面，每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(P83)

6 正则化
	正则化用来避免过拟合问题，思路是在损失函数中加入刻画模型复杂程度的指标。
	假设用于刻画模型在训练数据上表现的损失函数为J(θ)，那么优化时不是直接优化J(θ)，而是优化：
		J(θ)+λR(w)
	其中：
		θ：表示的是一个神经网络中所有的参数，包括边上的权重w和偏执项b。
		R(w)：刻画的是模型的复杂程度。
		λ：表示模型复杂损失在总损失中的比例。
	一般来说模型复杂度只由权重w决定。

	常用的刻画模型复杂度的函数R(w)有2种：
	1 L1正则化：R(w)=‖w‖1=∑︳wi︳
	2 L2正则化：R(w)

	不管哪种，基本思想都是希望通过限制权重的大小，是的模型不能任意拟合训练数据中的随机噪音。

	因为在优化时需要计算损失函数的偏导数，所以对含有L2正则化损失函数的优化要更加简洁。

7 滑动平均模型

