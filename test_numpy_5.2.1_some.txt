1 损失函数：
	使用有3种情况：
		# 后注：1 分类问题：一般采用“交叉熵+softmax回归”，见P75；
		# 后注：2 回归问题：一般采用“均方误差”，见P77；
		# 后注：3 自定义损失函数：还可以自定义损失函数，见P78

2 反向传播算法
	梯度下降算法主要用于优化单个参数的取值，而反向传播算法给出了一个高效的方式在所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小。
	反向传播算法会计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数。

3 梯度下降算法
	梯度其实就是损失函数当前所在的位置，梯度下降算法，对参数的梯度通过求偏导的方式计算出来，再根据学习率来更新参数。