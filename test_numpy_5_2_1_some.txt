0 归纳：
	1 前向传播算法（损失函数）是求损失的过程，会用到的
		1)交叉熵
		2)Softmax回归
		3)梯度下降算法
		4)随机梯度下降
		5)正则化（为防止优化过程造成过拟合，对参数进行正则化）

	2 反向传播算法是优化参数的过程，会用到：
		1)梯度下降算法
			梯度
			学习率
		2)随机梯度下降
		3)指数衰减法（反向传播算法的最终实现，先使用学习率，然后再使用优化函数来优化）
			学习率（learning rate，learning_rate = tf.train.exponential_decay）
			训练轮数(global_step，初始值设为0，然后在优化函数中可以被自动更新)
			优化函数(train_step = tf.train.GradientDescentOptimizer)

	3 前向传播算法（损失函数），反向传播算法，都是在一定的数据量上进行的，也就用到了：
		1)batch

1 损失函数：
	损失函数用来刻画预测值与真实值之间的差异，使用上有3种情况：
		1 分类问题：一般采用“交叉熵+softmax回归”，见P75；
		2 回归问题：一般采用“均方误差”，见P77；
		3 自定义损失函数：还可以自定义损失函数，见P78

2 交叉熵
	交叉熵（cross entropy）刻画了两个概率分布之间的距离，见P74。

3 Softmax回归
	Softmax回归用来将神经网络前向传播得到的结果转化成概率分布，见P75。

4 反向传播算法
	反向传播算法要用到损失函数，反向传播算法就是通过损失函数刻画出的测试值与真实值之间的差异，来对参数进行优化的。
	梯度下降算法主要用于优化单个参数的取值，而反向传播算法给出了一个高效的方式在所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小。
	反向传播算法会计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数，反向传播算法可以说是梯度下降在链式法则中的应用。

5 梯度下降算法(P80)
	梯度其实就是损失函数当前所在的位置，梯度下降算法，对参数的梯度通过求偏导的方式计算出来，再根据学习率来更新参数。
	这里面有两个概念：梯度，学习率。
	1 梯度：可以将梯度理解为斜率，通过求偏导得到。
	2 学习率：每次参数更新的幅度

6 随机梯度下降(P83)
	梯度下降算法，有两个缺点：1 不一定能够达到全局最优，2 计算时间太长。
	梯度下降算法，每轮训练会输入n次参数进行训练。如果每一轮训练都需要计算在全部训练数据上的损失函数，这是非常耗时的。
	随机梯度下降算法，优化的不是在全部训练数据上的损失函数，而是在每一轮迭代中，随机优化某一条训练数据上的损失函数。这样每一轮参数更新的速度大大加快。

7 batch(P83)
	随机梯度下降算法，每次优化的只是某一条数据上的损失函数，所以缺点也比较明显：
	在某一条数据上损失函数更新并不代表在全部数据上损失函数更小，于是使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优。

	于是为了综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中一般采用这两个算法的折中：
	每次计算一小部分训练数据的损失函数，这一小部分数据被称之为一个batch。

	通过矩阵运算，每次在一个batch上优化神经网络的参数并不会比单个数据慢太多。
	另一方面，每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(P83)

8 指数衰减法(P84)
	学习率（learning rate）：
		在训练神经网络的时候，需要设置学习率控制参数更新的速度。学习率既不能过大，也不能过小。
		过大时可能造成参数的摇摆，而不会收敛；过小时会降低优化速度。

	指数衰减法（exponential decay）：
		TensorFlow提供了tf.train.exponential_decay函数来实现指数衰减法学习率。
		通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使模型在训练后期更加稳定。
		exponential_decay函数会指数级的减小学习率，它实现了以下代码的功能：

			decayed_learning_rate = learning_rate * decay_rate ^ ( global_step / decay_steps )

		其中：
			decayed_learning_rate:	每一轮优化时使用的学习率
			learning_rate:			初始学习率
			decay_rate:				衰减系数
			global_step:			训练轮数
			decay_steps:			衰减速度。代表了完整的使用一遍训练数据所需要的迭代轮数。
									这个迭代轮数也就是总训练样本数除以每一个batch中的训练样本数。
									这种设置的常用场景是每完整地过完一遍训练数据，学习率就减小一次。见P85。

		TensorFlow的tf.exponential_decay函数：

			learning_rate = tf.train.exponential_decay(
				0.1, 					# 初始学习率
				global_step, 			# 当前迭代轮数
				100, 					# 过完所有训练数据需要的迭代轮数
				0.96, 					# 学习率衰减速度
				stainrcase = true		# 为true时global_step / decay_steps 会被转化为整数
				)
		
		然后就可以使用指数衰减的学习率了：

				learning_step = tf。train.GradientDescentOptimizer(learning_rate)	\
					.minimize(...my loss..., global_step = global_step)

		在minimize函数中传入global_step将自动更新global_step参数，从而使得学习率也得到相应更新。见P86。

9 正则化(P86)
	正则化用来避免过拟合问题，思路是在损失函数中加入刻画模型复杂程度的指标。
	假设用于刻画模型在训练数据上表现的损失函数为J(θ)，那么优化时不是直接优化J(θ)，而是优化：
		J(θ)+λR(w)
	其中：
		θ：表示的是一个神经网络中所有的参数，包括边上的权重w和偏执项b。
		R(w)：刻画的是模型的复杂程度。
		λ：表示模型复杂损失在总损失中的比例。
	一般来说模型复杂度只由权重w决定。

	常用的刻画模型复杂度的函数R(w)有2种：
	1 L1正则化：R(w)=‖w‖1=∑︳wi︳
	2 L2正则化：R(w)

	不管哪种，基本思想都是希望通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪音。

	因为在优化时需要计算损失函数的偏导数，所以对含有L2正则化损失函数的优化要更加简洁。

	直观上的感觉，L1正则化和L2正则化都是让参数的位置变化得不要太崎岖，变化尽量优雅舒缓一点。

10 滑动平均模型(P90)
	衰减率（decay）：
		decay用于控制模型更新的速度
	
	轮数（step，又num_updates）：	
		num_updates用于动态设置衰减率（decay）的大小。
		初始化ExponentialMovingAverage时，如果使用了这个参数，那么每次使用的衰减率将是：
									1 + num_updates
			decay = min { decay, -------------------- }
									10 + num_updates

	影子变量（shadow variable）：影子变量的初始值是相应变量的值，而每次运行变量更新时，影子变量的值会更新为：
		shadow_variable = decay * shadow_variable + (1 + decay) * variable

	在滑动平均模型运行的过程中，能够看出：
	1. num_updates（轮数）比较小的时候，decay主要是
		1 + num_updates
		————————--------
		10 + num_updates
	的值，也就是说decay比较小，而这样一来计算出来的shadow_variable（新的影子变量）受到decay的影响就比较小，而主要受到新变量的影响。
	2. 随着num_updates值的逐渐变大，decay逐渐变大，那么新生成的shadow_variable（新的影子变量）受到原有shadow_variable（原来的影子变量）的影响就比较大，也就是说shadow_variable趋于稳定，variable（新变量）只会小幅度的对其产生影响。

11 常用函数
	tf.cast					类型转换
	tf.equal				对比这两个矩阵或者向量的同样位置的元素，如果是相等的那就返回True，反正返回False。可与tf.cast合同计算概率
	tf.reduce_mean			求平均数，可以控制维度
	tf.argmax				返回最大值索引号
	tf.Variable				定义变量
	tf.random_normal		随机生成正态分布
	tf.truncated_normal		随机生成正态分布，但如果随机出来的值偏离平均值超过2个标准差，那么这个数将会被重新随机
	os.path.join()			拼接文件路径，可以穿入多个路径

	tf.get_variable						创建或者获取变量
	tf.veriable_scope					生成一个上下文管理器，一般会与tf.get_variable函数共用
	tf.truncated_normal_initializer		用于初始化变量，可以放在tf.get_variable函数中，见P107表5-2

11.1 卷积层

	tf.nn.conv2d			卷积层前向传播函数，见P146
							1.当前层节点矩阵（4维：batch，长度，宽度，深度）
							2.卷积层的权重信息（4维：长度，宽度，当前层深度，过滤器深度）
							3.多维度上的步长（4维：1，长度，宽度，1）
							4.填充方法（padding='SAME'维全0填充，padding='VALID'为不添加）

	tf.nn.bias_add			给前向传播的每一个节点加上偏置项，见P146
							1.卷积层前向传播方法
							2.偏置项

	tf.nn.relu				ReLU激活函数，用来去线性化

11.2 池化层

	tf.nn.max.pool			最大池化层的前向传播，参数与tf.nn.conv2d函数类似，见P148
							1.当前层的节点矩阵（4维：batch，长度，宽度，深度）
							2.过滤器尺寸（4维：用的较多的是[1，2，2，1]或[1, 3, 3, 1]）
							3.步长（4维：1，长度，宽度，1）
							4.填充方法（padding='SAME'维全0填充，padding='VALID'为不添加）


12 持久化
	saver.save(sess, "/path/to/model/model.ckpt")
	保存了3个文件：
	1 model.ckpt.mete
		它保存了TensorFlow计算图的结构。
	2 model.ckpt
		它保存了TensorFlow程序中每一个变量的取值。
	3 checkpoint
		它保存了一个目录下所有的模型文件列表。


